import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def scan_website(url):
    try:
        # ì›¹ í˜ì´ì§€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"ğŸ” ìŠ¤ìº” ì¤‘: {url}")
        
        # ëª¨ë“  ë§í¬ ê²€ì‚¬
        for link in soup.find_all('a', href=True):
            full_url = urljoin(url, link['href'])
            
            # ê¹¨ì§„ ë§í¬ í™•ì¸
            try:
                link_status = requests.head(full_url, allow_redirects=True).status_code
                if link_status == 404:
                    print(f"âŒ ê¹¨ì§„ ë§í¬ ë°œê²¬: {full_url}")
            except:
                print(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {full_url}")
            
            # ì™¸ë¶€ ë§í¬ ê²½ê³ 
            if link['href'].lower().startswith(('http://', 'https://')) and not url in link['href']:
                print(f"âš ï¸ ì™¸ë¶€ ë§í¬ (ì ì¬ì  ìœ„í—˜): {full_url}")

        # CSRF í† í°ì´ ì—†ëŠ” form í™•ì¸
        for form in soup.find_all('form'):
            if not form.find('input', {'type': 'hidden', 'name': 'csrf_token'}):
                print(f"ğŸš¨ CSRF ë³´í˜¸ ì—†ëŠ” ì–‘ì‹ ê°ì§€ë¨: {form}")

    except Exception as e:
        print(f"â— ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    target_url = input("ìŠ¤ìº”í•  URLì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: https://example.com): ")
    scan_website(target_url)
  Add main scanner script
